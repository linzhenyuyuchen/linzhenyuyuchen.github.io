---
layout:     post
title:      seq2seq
subtitle:   seq2seq
date:       2020-02-25
author:     LZY
header-img: img/xiaohuangren.jpg
catalog: true
tags:
    - RNN
    - NLP
---

# seq2seq

[Github Tutorials](https://github.com/bentrevett/pytorch-seq2seq)

Seq2Seq是Encoder-Decoder结构的网络,输入输出都是一个序列。Encoder和Decoder一般都是RNN (LSTM / GRU)

![](/img/V200.jpg)


![](/img/22325.png)

## Teacher Forcing

在训练过程中，直接使用要解码的序列(ground truth)作为输入进行训练，而不是上一阶段生成的预测

![](/img/22326.png)

## Attention

Attention机制就是让编码器编码出来的向量根据解码器要解码的东西动态变化的一种机制

![](/img/66251.png)

![](/img/66282.png)

## Beam Search

在inference阶段，不能使用Teacher Forcing，那么只能使用上一时刻解码的输出作为下一个解码的输入，这样会导致误差传递，怎么解决这个问题呢？

Beam Search：在每个时刻解码器都会选择Top k个预测结果作为下一个解码器的输入，将这K个结果逐一输入到解码器进行解码，就会产生k倍个预测结果，从所有的解码结果中再选出Top K个预测结果作为下一个解码器的输入。在最后一个时刻再选出Top 1作为最终的输出，这样减少的错误传递。

